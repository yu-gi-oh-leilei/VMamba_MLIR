MODE:
  name: vssm1_small_0229_fc
DATA:
  dataset_dir: '/media/data2/MLICdataset/'
  dataname: 'voc2007'
  num_workers: 8
  num_class: 20
  classnames: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
                  'bus', 'car', 'cat', 'chair', 'cow',
                  'diningtable', 'dog', 'horse', 'motorbike', 'person',
                  'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']
  

  TRANSFORM:
    img_size: 448
    crop: False
    cutout: True
    length: 224
    cut_fact: 0.5
    orid_norm: False
    remove_norm: False
    n_holes: 1

INPUT:
  SIZE: (448, 448)
  INTERPOLATION: "bicubic"
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  TRANSFORMS: ["random_resized_crop", "MLC_Policy", "random_flip", "normalize"]
  TRANSFORMS_TEST: ["resize", "normalize"]
  random_resized_crop_scale: (0.5, 1.0)

MODEL:
  arch: 'vssm1_small_0229'
  use_BN: False
  TYPE: vssm
  NAME: vssm1_small_0229
  DROP_PATH_RATE: 0.3
  NUM_CLASSES: 1000
  VSSM:
    is_pretrain: '/mnt/disk2/maleilei/pretrain/vssm_small_0229_ckpt_epoch_222.pth'
    PATCH_SIZE: 4
    IN_CHANS: 3
    SSM_RANK_RATIO: 2.0
    SSM_ACT_LAYER: "silu"
    SSM_DROP_RATE: 0.0
    SSM_INIT: "v0"
    MLP_ACT_LAYER: "gelu"
    MLP_DROP_RATE: 0.0
    PATCH_NORM: True
    NORM_LAYER: "ln"
    GMLP: False
    
    EMBED_DIM: 96
    DEPTHS: [ 2, 2, 15, 2 ]
    SSM_D_STATE: 1
    SSM_DT_RANK: "auto"
    SSM_RATIO: 2.0
    SSM_CONV: 3
    SSM_CONV_BIAS: false
    SSM_FORWARDTYPE: "v3noz"
    MLP_RATIO: 4.0
    DOWNSAMPLE: "v3"
    PATCHEMBED: "v2"
  BACKBONE:
    backbone: 'ViT-B/16'
    pretrained: True
    frozen_backbone: False
  TRANSFORMER:
    enc_layers: 0
    dec_layers: 1
    dim_feedforward: 2048
    hidden_dim: 512
    dropout: 0.1
    nheads: 4
    pre_norm: False
    position_embedding: 'v2'
    keep_other_self_attn_dec: False
    keep_first_self_attn_dec: False
    keep_input_proj: False
  CLASSIFIER:
    num_class: 20
  CAPTION:
    n_ctx: 16
    csc: True
    ctx_init: ""
    class_token_position: "end"
    gl_merge_rate: 0.5


LOSS:
  loss_mode: asl # asl, multi_bce
  loss_dev: -1
  ASL:
    eps: 1e-05
    dtgfl: True
    gamma_pos: 0.0
    gamma_neg: 2.0
    loss_clip: 0.0
  Coef:
    cls_asl_coef: 1.0
    cls_kcr_coef: 4.0
    # cls_bce_coef: 1.0
    # div_coef: 1.0

OPTIMIZER:
  optim: 'AdamW'
  lr_scheduler: 'OneCycleLR'
  pattern_parameters: 'single_lr'
  momentum: 0.9
  warmup_epoch: 5
  warmup_multiplier: 50
  warmup_scheduler: False
  epoch_step: [10, 20]

  batch_size: 64
  lr: 2e-4
  lrp: 0.1
  weight_decay: 1e-2




DDP:
  world_size: 1
  rank: 0
  dist_url: 'tcp://127.0.0.1:3722'
  local_rank: 1

TRAIN:
  seed: 1
  amp: True
  early_stop: True
  kill_stop: True
  device: 'CUDA'
  start_epoch: 0
  epochs: 80 # 40
  ema_decay: 0.9997
  ema_epoch: 0
  ratio: 2.0
  evaluate: False
  USE_CHECKPOINT: False

INPUT_OUTPUT:
  output: ''
  resume: ''
  resume_omit: []
  print_freq: 100
  out_aps: False

EVAL:
  val_interval: 1
  val_epoch_start: 0
  # lr: 1.0
  # base_lr: 0.1
  # base_lr: 1.0
  # lr:  1e-4
  # lr: 5e-04
  # lr: 5e-03

  # batch_size: 16
  # lr: 1e-04

  # batch_size: 256
  # lr: 5e-04
  
  # batch_size: 128
  # lr: 2e-4

  # OPTIMIZER:
#   optim: 'SGD' # SGD
#   lr_scheduler: 'MultiStepLR'
#   pattern_parameters: 'mutil_lr'
#   momentum: 0.9
#   warmup_epoch: 5
#   warmup_multiplier: 50
#   warmup_scheduler: True
#   epoch_step: [4, 8, 12, 16, 20, 24, 28]
#   # lr: 1.0
#   # base_lr: 0.1
#   # base_lr: 1.0
#   # lr:  1e-4
#   batch_size: 16
#   lr: 0.01
#   lrp: 0.1
#   weight_decay: 1e-4
